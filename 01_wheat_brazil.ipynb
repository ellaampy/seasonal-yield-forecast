{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models.vanillann import YieldDataset, SimpleModel\n",
    "from sklearn.feature_selection  import mutual_info_regression\n",
    "\n",
    "ecmwf_path = \"data/preprocessed/US/ecmwf_era_wheat_US.csv\"\n",
    "predictor_path = \"data/preprocessed/US/ndvi_soil_soil_moisture_meteo_fpar_wheat_US.csv\"\n",
    "yield_path = \"data/CY-Bench/US/wheat/yield_wheat_US.csv\"\n",
    "test_years = [2015, 2018, 2022]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yield_and_predictors(yield_path, predictor_path, ecmwf_path, test_years):\n",
    "    y = pd.read_csv(yield_path)\n",
    "    y = y.loc[y[\"harvest_year\"].between(2003, 2023), [\"adm_id\", \"harvest_year\", \"yield\"]].reset_index(drop=True)\n",
    "\n",
    "    # Merge predictor data\n",
    "    x_1 = pd.read_csv(ecmwf_path)\n",
    "    x_2 = pd.read_csv(predictor_path)\n",
    "    x = x_1.merge(x_2, on=[\"adm_id\", \"harvest_year\"], how=\"left\").dropna().reset_index(drop=True)\n",
    "\n",
    "    # Merge predictor and yield data\n",
    "    x_y = x.merge(y, on=[\"adm_id\", \"harvest_year\"], how=\"inner\")\n",
    "\n",
    "    train_df = x_y[~x_y['harvest_year'].isin(test_years)].reset_index(drop=True)\n",
    "    test_df = x_y[x_y['harvest_year'].isin(test_years)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = get_yield_and_predictors(yield_path, predictor_path, ecmwf_path, test_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(train_df[\"init_date\"]).dt.month.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_season_df = train_df[pd.to_datetime(train_df[\"init_date\"]).dt.month == 9].drop(columns=[\"init_date\"]).set_index(\"adm_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(yhat,y):\n",
    "    return 100 * torch.sqrt(torch.mean((yhat-y)**2)) / torch.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_season_df = end_of_season_df.set_index([\"yield\", \"harvest_year\"], append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_season_df = (end_of_season_df[[c for c in end_of_season_df.columns if \"tmax\" in c]].rolling(window=4, step=4, axis=1).mean().dropna(axis=1)\n",
    " .join(\n",
    "     end_of_season_df[[c for c in end_of_season_df.columns if \"tmin\" in c]].rolling(window=4, step=4, axis=1).mean().dropna(axis=1)\n",
    " ).join(\n",
    "    end_of_season_df[[c for c in end_of_season_df.columns if \"prec\" in c]].rolling(window=4, step=4, axis=1).mean().dropna(axis=1)\n",
    " ).join(\n",
    "     end_of_season_df[[\"awc\", \"bulk_density\", \"drainage_class_1\", \"drainage_class_2\", \"drainage_class_3\", \"drainage_class_4\", \"drainage_class_5\", \"drainage_class_6\"]]\n",
    " ).reset_index().set_index(\"adm_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yield</th>\n",
       "      <th>harvest_year</th>\n",
       "      <th>tmax_8</th>\n",
       "      <th>tmax_12</th>\n",
       "      <th>tmax_16</th>\n",
       "      <th>tmax_20</th>\n",
       "      <th>tmax_24</th>\n",
       "      <th>tmax_28</th>\n",
       "      <th>tmin_8</th>\n",
       "      <th>tmin_12</th>\n",
       "      <th>...</th>\n",
       "      <th>prec_24</th>\n",
       "      <th>prec_28</th>\n",
       "      <th>awc</th>\n",
       "      <th>bulk_density</th>\n",
       "      <th>drainage_class_1</th>\n",
       "      <th>drainage_class_2</th>\n",
       "      <th>drainage_class_3</th>\n",
       "      <th>drainage_class_4</th>\n",
       "      <th>drainage_class_5</th>\n",
       "      <th>drainage_class_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adm_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US-01-001</th>\n",
       "      <td>3.9707</td>\n",
       "      <td>2003</td>\n",
       "      <td>17.270446</td>\n",
       "      <td>20.937684</td>\n",
       "      <td>26.825161</td>\n",
       "      <td>27.870006</td>\n",
       "      <td>29.791414</td>\n",
       "      <td>30.027308</td>\n",
       "      <td>6.340036</td>\n",
       "      <td>10.441741</td>\n",
       "      <td>...</td>\n",
       "      <td>5.243151</td>\n",
       "      <td>5.574161</td>\n",
       "      <td>11.436359</td>\n",
       "      <td>1.342325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-01-003</th>\n",
       "      <td>3.1631</td>\n",
       "      <td>2003</td>\n",
       "      <td>18.446471</td>\n",
       "      <td>21.386636</td>\n",
       "      <td>26.738730</td>\n",
       "      <td>28.084772</td>\n",
       "      <td>29.090873</td>\n",
       "      <td>29.618936</td>\n",
       "      <td>9.253883</td>\n",
       "      <td>12.043147</td>\n",
       "      <td>...</td>\n",
       "      <td>9.176004</td>\n",
       "      <td>6.633327</td>\n",
       "      <td>14.818747</td>\n",
       "      <td>1.424239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-01-003</th>\n",
       "      <td>3.3650</td>\n",
       "      <td>2004</td>\n",
       "      <td>17.260808</td>\n",
       "      <td>22.149763</td>\n",
       "      <td>25.288360</td>\n",
       "      <td>29.233394</td>\n",
       "      <td>30.445200</td>\n",
       "      <td>30.656131</td>\n",
       "      <td>8.757897</td>\n",
       "      <td>11.588240</td>\n",
       "      <td>...</td>\n",
       "      <td>5.335587</td>\n",
       "      <td>4.157435</td>\n",
       "      <td>14.818747</td>\n",
       "      <td>1.424239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-01-003</th>\n",
       "      <td>3.4323</td>\n",
       "      <td>2006</td>\n",
       "      <td>19.049926</td>\n",
       "      <td>22.990201</td>\n",
       "      <td>26.594140</td>\n",
       "      <td>31.007885</td>\n",
       "      <td>32.452603</td>\n",
       "      <td>32.025328</td>\n",
       "      <td>9.436357</td>\n",
       "      <td>12.178329</td>\n",
       "      <td>...</td>\n",
       "      <td>1.656067</td>\n",
       "      <td>3.771095</td>\n",
       "      <td>14.818747</td>\n",
       "      <td>1.424239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-01-003</th>\n",
       "      <td>3.6342</td>\n",
       "      <td>2007</td>\n",
       "      <td>18.125727</td>\n",
       "      <td>22.861901</td>\n",
       "      <td>27.243667</td>\n",
       "      <td>29.355054</td>\n",
       "      <td>31.427483</td>\n",
       "      <td>32.693986</td>\n",
       "      <td>7.138254</td>\n",
       "      <td>13.386155</td>\n",
       "      <td>...</td>\n",
       "      <td>3.823193</td>\n",
       "      <td>2.309769</td>\n",
       "      <td>14.818747</td>\n",
       "      <td>1.424239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-56-033</th>\n",
       "      <td>2.0863</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.233752</td>\n",
       "      <td>10.902196</td>\n",
       "      <td>13.228545</td>\n",
       "      <td>20.573137</td>\n",
       "      <td>27.212852</td>\n",
       "      <td>32.217816</td>\n",
       "      <td>-12.818909</td>\n",
       "      <td>-1.986878</td>\n",
       "      <td>...</td>\n",
       "      <td>1.811108</td>\n",
       "      <td>0.582011</td>\n",
       "      <td>13.133602</td>\n",
       "      <td>1.559058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-56-033</th>\n",
       "      <td>1.6152</td>\n",
       "      <td>2004</td>\n",
       "      <td>5.824528</td>\n",
       "      <td>13.768752</td>\n",
       "      <td>16.756334</td>\n",
       "      <td>19.084380</td>\n",
       "      <td>24.497442</td>\n",
       "      <td>25.945786</td>\n",
       "      <td>-6.548714</td>\n",
       "      <td>0.661033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.836529</td>\n",
       "      <td>2.263215</td>\n",
       "      <td>13.133602</td>\n",
       "      <td>1.559058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-56-033</th>\n",
       "      <td>3.1631</td>\n",
       "      <td>2005</td>\n",
       "      <td>7.939766</td>\n",
       "      <td>11.219689</td>\n",
       "      <td>12.008681</td>\n",
       "      <td>19.030050</td>\n",
       "      <td>28.913044</td>\n",
       "      <td>27.521570</td>\n",
       "      <td>-4.799025</td>\n",
       "      <td>-2.013270</td>\n",
       "      <td>...</td>\n",
       "      <td>1.241215</td>\n",
       "      <td>2.053632</td>\n",
       "      <td>13.133602</td>\n",
       "      <td>1.559058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-56-033</th>\n",
       "      <td>2.0190</td>\n",
       "      <td>2007</td>\n",
       "      <td>6.420172</td>\n",
       "      <td>8.795595</td>\n",
       "      <td>18.362682</td>\n",
       "      <td>19.661393</td>\n",
       "      <td>30.077782</td>\n",
       "      <td>31.287195</td>\n",
       "      <td>-6.789862</td>\n",
       "      <td>-2.890519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430370</td>\n",
       "      <td>0.953817</td>\n",
       "      <td>13.133602</td>\n",
       "      <td>1.559058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US-56-033</th>\n",
       "      <td>2.8266</td>\n",
       "      <td>2008</td>\n",
       "      <td>5.184251</td>\n",
       "      <td>6.811825</td>\n",
       "      <td>12.889855</td>\n",
       "      <td>17.741680</td>\n",
       "      <td>26.385360</td>\n",
       "      <td>29.286043</td>\n",
       "      <td>-6.775330</td>\n",
       "      <td>-4.098244</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221454</td>\n",
       "      <td>0.551057</td>\n",
       "      <td>13.133602</td>\n",
       "      <td>1.559058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21054 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            yield  harvest_year     tmax_8    tmax_12    tmax_16    tmax_20  \\\n",
       "adm_id                                                                        \n",
       "US-01-001  3.9707          2003  17.270446  20.937684  26.825161  27.870006   \n",
       "US-01-003  3.1631          2003  18.446471  21.386636  26.738730  28.084772   \n",
       "US-01-003  3.3650          2004  17.260808  22.149763  25.288360  29.233394   \n",
       "US-01-003  3.4323          2006  19.049926  22.990201  26.594140  31.007885   \n",
       "US-01-003  3.6342          2007  18.125727  22.861901  27.243667  29.355054   \n",
       "...           ...           ...        ...        ...        ...        ...   \n",
       "US-56-033  2.0863          2003   0.233752  10.902196  13.228545  20.573137   \n",
       "US-56-033  1.6152          2004   5.824528  13.768752  16.756334  19.084380   \n",
       "US-56-033  3.1631          2005   7.939766  11.219689  12.008681  19.030050   \n",
       "US-56-033  2.0190          2007   6.420172   8.795595  18.362682  19.661393   \n",
       "US-56-033  2.8266          2008   5.184251   6.811825  12.889855  17.741680   \n",
       "\n",
       "             tmax_24    tmax_28     tmin_8    tmin_12  ...   prec_24  \\\n",
       "adm_id                                                 ...             \n",
       "US-01-001  29.791414  30.027308   6.340036  10.441741  ...  5.243151   \n",
       "US-01-003  29.090873  29.618936   9.253883  12.043147  ...  9.176004   \n",
       "US-01-003  30.445200  30.656131   8.757897  11.588240  ...  5.335587   \n",
       "US-01-003  32.452603  32.025328   9.436357  12.178329  ...  1.656067   \n",
       "US-01-003  31.427483  32.693986   7.138254  13.386155  ...  3.823193   \n",
       "...              ...        ...        ...        ...  ...       ...   \n",
       "US-56-033  27.212852  32.217816 -12.818909  -1.986878  ...  1.811108   \n",
       "US-56-033  24.497442  25.945786  -6.548714   0.661033  ...  1.836529   \n",
       "US-56-033  28.913044  27.521570  -4.799025  -2.013270  ...  1.241215   \n",
       "US-56-033  30.077782  31.287195  -6.789862  -2.890519  ...  0.430370   \n",
       "US-56-033  26.385360  29.286043  -6.775330  -4.098244  ...  1.221454   \n",
       "\n",
       "            prec_28        awc  bulk_density  drainage_class_1  \\\n",
       "adm_id                                                           \n",
       "US-01-001  5.574161  11.436359      1.342325               0.0   \n",
       "US-01-003  6.633327  14.818747      1.424239               0.0   \n",
       "US-01-003  4.157435  14.818747      1.424239               0.0   \n",
       "US-01-003  3.771095  14.818747      1.424239               0.0   \n",
       "US-01-003  2.309769  14.818747      1.424239               0.0   \n",
       "...             ...        ...           ...               ...   \n",
       "US-56-033  0.582011  13.133602      1.559058               0.0   \n",
       "US-56-033  2.263215  13.133602      1.559058               0.0   \n",
       "US-56-033  2.053632  13.133602      1.559058               0.0   \n",
       "US-56-033  0.953817  13.133602      1.559058               0.0   \n",
       "US-56-033  0.551057  13.133602      1.559058               0.0   \n",
       "\n",
       "           drainage_class_2  drainage_class_3  drainage_class_4  \\\n",
       "adm_id                                                            \n",
       "US-01-001               0.0               0.0               1.0   \n",
       "US-01-003               0.0               0.0               0.0   \n",
       "US-01-003               0.0               0.0               0.0   \n",
       "US-01-003               0.0               0.0               0.0   \n",
       "US-01-003               0.0               0.0               0.0   \n",
       "...                     ...               ...               ...   \n",
       "US-56-033               0.0               0.0               0.0   \n",
       "US-56-033               0.0               0.0               0.0   \n",
       "US-56-033               0.0               0.0               0.0   \n",
       "US-56-033               0.0               0.0               0.0   \n",
       "US-56-033               0.0               0.0               0.0   \n",
       "\n",
       "           drainage_class_5  drainage_class_6  \n",
       "adm_id                                         \n",
       "US-01-001               0.0               0.0  \n",
       "US-01-003               1.0               0.0  \n",
       "US-01-003               1.0               0.0  \n",
       "US-01-003               1.0               0.0  \n",
       "US-01-003               1.0               0.0  \n",
       "...                     ...               ...  \n",
       "US-56-033               1.0               0.0  \n",
       "US-56-033               1.0               0.0  \n",
       "US-56-033               1.0               0.0  \n",
       "US-56-033               1.0               0.0  \n",
       "US-56-033               1.0               0.0  \n",
       "\n",
       "[21054 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_of_season_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating on year 2003\n",
      "Epoch 1, Validation Loss for year 2003: 43.57643826802572\n",
      "Epoch 2, Validation Loss for year 2003: 36.37793646918403\n",
      "Epoch 3, Validation Loss for year 2003: 34.92403881638138\n",
      "Epoch 4, Validation Loss for year 2003: 33.85168983318187\n",
      "Epoch 5, Validation Loss for year 2003: 33.79119788275825\n",
      "Epoch 6, Validation Loss for year 2003: 33.34014613540084\n",
      "Epoch 7, Validation Loss for year 2003: 32.63258859846327\n",
      "Epoch 8, Validation Loss for year 2003: 32.77996197453252\n",
      "Epoch 9, Validation Loss for year 2003: 32.512628625940394\n",
      "Epoch 10, Validation Loss for year 2003: 32.75680167586715\n",
      "Validating on year 2004\n",
      "Epoch 1, Validation Loss for year 2004: 37.70468572469858\n",
      "Epoch 2, Validation Loss for year 2004: 33.60248455634484\n",
      "Epoch 3, Validation Loss for year 2004: 34.17192598489615\n",
      "Epoch 4, Validation Loss for year 2004: 34.08541811429537\n",
      "Epoch 5, Validation Loss for year 2004: 34.36715419475849\n",
      "Epoch 6, Validation Loss for year 2004: 32.787856542147125\n",
      "Epoch 7, Validation Loss for year 2004: 34.92587834138136\n",
      "Epoch 8, Validation Loss for year 2004: 35.125277702624984\n",
      "Epoch 9, Validation Loss for year 2004: 34.76776893322285\n",
      "Epoch 10, Validation Loss for year 2004: 34.00419734074519\n",
      "Early stopping at epoch 10\n",
      "Validating on year 2005\n",
      "Epoch 1, Validation Loss for year 2005: 34.36176277160644\n",
      "Epoch 2, Validation Loss for year 2005: 28.523952102661134\n",
      "Epoch 3, Validation Loss for year 2005: 27.442607955932615\n",
      "Epoch 4, Validation Loss for year 2005: 26.491219139099123\n",
      "Epoch 5, Validation Loss for year 2005: 26.11359817504883\n",
      "Epoch 6, Validation Loss for year 2005: 25.927645721435546\n",
      "Epoch 7, Validation Loss for year 2005: 25.506639785766602\n",
      "Epoch 8, Validation Loss for year 2005: 25.087075080871582\n",
      "Epoch 9, Validation Loss for year 2005: 25.184344329833984\n",
      "Epoch 10, Validation Loss for year 2005: 24.65101535797119\n",
      "Validating on year 2006\n",
      "Epoch 1, Validation Loss for year 2006: 44.08767890930176\n",
      "Epoch 2, Validation Loss for year 2006: 36.534795913696286\n",
      "Epoch 3, Validation Loss for year 2006: 33.75504570007324\n",
      "Epoch 4, Validation Loss for year 2006: 32.48159622192383\n",
      "Epoch 5, Validation Loss for year 2006: 31.411849060058593\n",
      "Epoch 6, Validation Loss for year 2006: 30.636360931396485\n",
      "Epoch 7, Validation Loss for year 2006: 30.323585815429688\n",
      "Epoch 8, Validation Loss for year 2006: 29.50255874633789\n",
      "Epoch 9, Validation Loss for year 2006: 29.080228424072267\n",
      "Epoch 10, Validation Loss for year 2006: 29.030554962158202\n",
      "Validating on year 2007\n",
      "Epoch 1, Validation Loss for year 2007: 37.76206169128418\n",
      "Epoch 2, Validation Loss for year 2007: 36.5390811920166\n",
      "Epoch 3, Validation Loss for year 2007: 36.68387275695801\n",
      "Epoch 4, Validation Loss for year 2007: 37.76137466430664\n",
      "Epoch 5, Validation Loss for year 2007: 37.25908317565918\n",
      "Epoch 6, Validation Loss for year 2007: 36.126101913452146\n",
      "Epoch 7, Validation Loss for year 2007: 38.12152809143066\n",
      "Epoch 8, Validation Loss for year 2007: 36.79344947814941\n",
      "Epoch 9, Validation Loss for year 2007: 37.55048141479492\n",
      "Epoch 10, Validation Loss for year 2007: 38.10472190856934\n",
      "Early stopping at epoch 10\n",
      "Validating on year 2008\n",
      "Epoch 1, Validation Loss for year 2008: 35.81464485895066\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     49\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 50\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Max Zachow\\anaconda3\\envs\\global-yield-forecast\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Max Zachow\\anaconda3\\envs\\global-yield-forecast\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Max Zachow\\anaconda3\\envs\\global-yield-forecast\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Max Zachow\\anaconda3\\envs\\global-yield-forecast\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max Zachow\\anaconda3\\envs\\global-yield-forecast\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unique_years = end_of_season_df['harvest_year'].unique()\n",
    "unique_years.sort()\n",
    "results = dict.fromkeys(unique_years)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for year in unique_years:\n",
    "    print(f'Validating on year {year}')\n",
    "    \n",
    "    # Create training and validation sets for this fold\n",
    "    train_fold_df = end_of_season_df[end_of_season_df['harvest_year'] != year]\n",
    "    val_fold_df = end_of_season_df[end_of_season_df['harvest_year'] == year]\n",
    "    \n",
    "    train_fold_features = train_fold_df.drop(columns=['yield', 'harvest_year'])\n",
    "    train_fold_target = train_fold_df['yield']\n",
    "    val_fold_features = val_fold_df.drop(columns=['yield', 'harvest_year'])\n",
    "    val_fold_target = val_fold_df['yield']\n",
    "    \n",
    "    means = train_fold_features.mean()\n",
    "    stds = train_fold_features.std()\n",
    "    train_fold_features = (train_fold_features - means) / stds\n",
    "    val_fold_features = (val_fold_features - means) / stds\n",
    "    \n",
    "    \n",
    "    train_fold_dataset = YieldDataset(train_fold_features, train_fold_target)\n",
    "    val_fold_dataset = YieldDataset(val_fold_features, val_fold_target)\n",
    "    \n",
    "    train_fold_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_fold_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Reset the model and optimizer\n",
    "    model = SimpleModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = RMSELoss\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    num_epochs = 10  # Set the maximum number of epochs you want to train for\n",
    "    patience = 4  # Number of epochs to wait for improvement before stopping\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    epochs_no_improve = 0  # Counter for epochs without improvement\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, target in train_fold_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features)\n",
    "            loss = criterion(output, target.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, target in val_fold_loader:\n",
    "                output = model(features)\n",
    "                loss = criterion(output, target.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_fold_loader)  # Compute the average validation loss\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss for year {year}: {val_loss}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0  # Reset the counter if validation loss improves\n",
    "        else:\n",
    "            epochs_no_improve += 1  # Increment the counter if validation loss does not improve\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break  # Stop training if no improvement for specified number of epochs\n",
    "    \n",
    "    results[year] = best_val_loss\n",
    "        \n",
    "# Once cross-validation is done, you can test on the test dataset using test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-yield-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
