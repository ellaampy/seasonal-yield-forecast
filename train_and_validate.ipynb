{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models.vanillann import YieldDataset, SimpleModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify use case through crop and country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR wheat data/preprocessed/BR/ecmwf_era_wheat_BR.csv data/CY-Bench/BR/wheat/yield_wheat_BR.csv [2006, 2015, 2017]\n"
     ]
    }
   ],
   "source": [
    "# USER INPUTS\n",
    "country = \"BR\" # one of [\"US\", \"BR\"]\n",
    "crop = \"wheat\" # one of [\"maize\", \"wheat\"]\n",
    "\n",
    "match country:\n",
    "    case \"US\":\n",
    "        match crop:\n",
    "            case \"wheat\":\n",
    "                ecmwf_path = \"data/preprocessed/US/ecmwf_era_wheat_US.csv\"\n",
    "                yield_path = \"data/CY-Bench/US/wheat/yield_wheat_US.csv\"\n",
    "                test_years = [2020, 2021, 2022]\n",
    "                print(country, crop, ecmwf_path, yield_path, test_years)\n",
    "            case \"maize\":\n",
    "                ecmwf_path = \"data/preprocessed/US/ecmwf_era_maize_US.csv\"\n",
    "                yield_path = \"data/CY-Bench/US/maize/yield_maize_US.csv\"\n",
    "                test_years = [2020, 2021, 2022]\n",
    "                print(country, crop, ecmwf_path, yield_path, test_years)\n",
    "            case _:\n",
    "                print(\"invalid crop, has to be wheat or maize\")\n",
    "    case \"BR\":\n",
    "        match crop:\n",
    "            case \"wheat\":\n",
    "                ecmwf_path = \"data/preprocessed/BR/ecmwf_era_wheat_BR.csv\"\n",
    "                yield_path = \"data/CY-Bench/BR/wheat/yield_wheat_BR.csv\"\n",
    "                test_years = [2006, 2015, 2017]\n",
    "                print(country, crop, ecmwf_path, yield_path, test_years)\n",
    "            case \"maize\":\n",
    "                ecmwf_path = \"data/preprocessed/BR/ecmwf_era_maize_BR.csv\"\n",
    "                yield_path = \"data/CY-Bench/BR/maize/yield_maize_BR.csv\"\n",
    "                test_years = [2020, 2021, 2022]\n",
    "                print(country, crop, ecmwf_path, yield_path, test_years)\n",
    "            case _:\n",
    "                print(\"invalid crop, has to be wheat or maize\")\n",
    "    case _:\n",
    "        print(\"invalid country, has to be US or BR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read yield and SCM_ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adm_id</th>\n",
       "      <th>harvest_year</th>\n",
       "      <th>init_month</th>\n",
       "      <th>init_time_step</th>\n",
       "      <th>tavg_16</th>\n",
       "      <th>tavg_17</th>\n",
       "      <th>tavg_18</th>\n",
       "      <th>tavg_19</th>\n",
       "      <th>tavg_20</th>\n",
       "      <th>tavg_21</th>\n",
       "      <th>...</th>\n",
       "      <th>tmin_40</th>\n",
       "      <th>tmin_41</th>\n",
       "      <th>tmax_39</th>\n",
       "      <th>tmax_40</th>\n",
       "      <th>tmax_41</th>\n",
       "      <th>prec_39</th>\n",
       "      <th>prec_40</th>\n",
       "      <th>prec_41</th>\n",
       "      <th>yield</th>\n",
       "      <th>harvested_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BR2311504</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>25.653409</td>\n",
       "      <td>25.939799</td>\n",
       "      <td>26.185234</td>\n",
       "      <td>26.433421</td>\n",
       "      <td>26.679016</td>\n",
       "      <td>26.861418</td>\n",
       "      <td>...</td>\n",
       "      <td>24.462146</td>\n",
       "      <td>24.512708</td>\n",
       "      <td>35.406395</td>\n",
       "      <td>35.381923</td>\n",
       "      <td>35.324830</td>\n",
       "      <td>0.217376</td>\n",
       "      <td>0.245209</td>\n",
       "      <td>0.310888</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BR2311504</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>26.782667</td>\n",
       "      <td>26.057738</td>\n",
       "      <td>26.166207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.261646</td>\n",
       "      <td>26.209929</td>\n",
       "      <td>...</td>\n",
       "      <td>24.313246</td>\n",
       "      <td>24.508361</td>\n",
       "      <td>35.201305</td>\n",
       "      <td>35.248513</td>\n",
       "      <td>35.284314</td>\n",
       "      <td>0.299057</td>\n",
       "      <td>0.232168</td>\n",
       "      <td>0.355174</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BR2311504</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>26.782667</td>\n",
       "      <td>26.057738</td>\n",
       "      <td>26.166207</td>\n",
       "      <td>26.364367</td>\n",
       "      <td>25.930489</td>\n",
       "      <td>26.029086</td>\n",
       "      <td>...</td>\n",
       "      <td>24.263344</td>\n",
       "      <td>24.303319</td>\n",
       "      <td>35.123788</td>\n",
       "      <td>35.311503</td>\n",
       "      <td>35.331876</td>\n",
       "      <td>0.182081</td>\n",
       "      <td>0.180712</td>\n",
       "      <td>0.199467</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BR2311504</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>26.782667</td>\n",
       "      <td>26.057738</td>\n",
       "      <td>26.166207</td>\n",
       "      <td>26.364367</td>\n",
       "      <td>25.930489</td>\n",
       "      <td>26.029086</td>\n",
       "      <td>...</td>\n",
       "      <td>24.127901</td>\n",
       "      <td>24.171361</td>\n",
       "      <td>35.140251</td>\n",
       "      <td>35.033899</td>\n",
       "      <td>34.994636</td>\n",
       "      <td>0.166692</td>\n",
       "      <td>0.349271</td>\n",
       "      <td>0.295164</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BR2311504</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>26.782667</td>\n",
       "      <td>26.057738</td>\n",
       "      <td>26.166207</td>\n",
       "      <td>26.364367</td>\n",
       "      <td>25.930489</td>\n",
       "      <td>26.029086</td>\n",
       "      <td>...</td>\n",
       "      <td>23.894624</td>\n",
       "      <td>24.096629</td>\n",
       "      <td>35.106095</td>\n",
       "      <td>35.052865</td>\n",
       "      <td>35.157488</td>\n",
       "      <td>0.136966</td>\n",
       "      <td>0.140238</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      adm_id  harvest_year  init_month  init_time_step    tavg_16    tavg_17  \\\n",
       "0  BR2311504          2020           5              16  25.653409  25.939799   \n",
       "1  BR2311504          2020           6              19  26.782667  26.057738   \n",
       "2  BR2311504          2020           7              23  26.782667  26.057738   \n",
       "3  BR2311504          2020           8              27  26.782667  26.057738   \n",
       "4  BR2311504          2020           9              31  26.782667  26.057738   \n",
       "\n",
       "     tavg_18    tavg_19    tavg_20    tavg_21  ...    tmin_40    tmin_41  \\\n",
       "0  26.185234  26.433421  26.679016  26.861418  ...  24.462146  24.512708   \n",
       "1  26.166207        NaN  25.261646  26.209929  ...  24.313246  24.508361   \n",
       "2  26.166207  26.364367  25.930489  26.029086  ...  24.263344  24.303319   \n",
       "3  26.166207  26.364367  25.930489  26.029086  ...  24.127901  24.171361   \n",
       "4  26.166207  26.364367  25.930489  26.029086  ...  23.894624  24.096629   \n",
       "\n",
       "     tmax_39    tmax_40    tmax_41   prec_39   prec_40   prec_41  yield  \\\n",
       "0  35.406395  35.381923  35.324830  0.217376  0.245209  0.310888    5.4   \n",
       "1  35.201305  35.248513  35.284314  0.299057  0.232168  0.355174    5.4   \n",
       "2  35.123788  35.311503  35.331876  0.182081  0.180712  0.199467    5.4   \n",
       "3  35.140251  35.033899  34.994636  0.166692  0.349271  0.295164    5.4   \n",
       "4  35.106095  35.052865  35.157488  0.136966  0.140238  0.163872    5.4   \n",
       "\n",
       "   harvested_area  \n",
       "0             5.0  \n",
       "1             5.0  \n",
       "2             5.0  \n",
       "3             5.0  \n",
       "4             5.0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Historical yield data\n",
    "y = pd.read_csv(yield_path)\n",
    "y = y.loc[y[\"harvest_year\"].between(2003, 2023), [\"adm_id\", \"harvest_year\", \"yield\", \"harvested_area\"]].reset_index(drop=True)\n",
    "\n",
    "# 8-day aggregated ECMWF and ERA data depending on month of initialization\n",
    "x = pd.read_csv(ecmwf_path)\n",
    "x_y = x.merge(y, on=[\"adm_id\", \"harvest_year\"], how=\"inner\")\n",
    "\n",
    "# remove test years\n",
    "train_df = x_y[~x_y['harvest_year'].isin(test_years)].reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for m in [5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "#for m in [9]:\n",
    "#train_df_scm = train_df[train_df[\"init_month\"] == 10].reset_index(drop=True)\n",
    "#first_scm_time_step = train_df_scm[\"init_time_step\"].max()\n",
    "#train_df_truncated = train_df[train_df[\"init_month\"] == 12].reset_index(drop=True)\n",
    "#train_df_truncated[[c for c in [l for l in train_df_truncated.columns if (\"tavg\" in l) or (\"tmax\" in l) or (\"tmin\" in l) or (\"prec\" in l)] if int(c.split(\"_\")[-1]) >= first_scm_time_step]] = (train_df_truncated\n",
    "# .groupby([\"adm_id\"])\n",
    "# [[c for c in [l for l in train_df_truncated.columns if (\"tavg\" in l) or (\"tmax\" in l) or (\"tmin\" in l) or (\"prec\" in l)] if int(c.split(\"_\")[-1]) >= first_scm_time_step]]\n",
    "# .transform(\"mean\"))\n",
    "\n",
    "#train_df_scm = train_df_scm.drop(columns=[\"init_month\", \"init_time_step\", \"production\", \"harvested_area\", \"planted_area\"]).set_index(\"adm_id\")\n",
    "#train_df_truncated = train_df_truncated.drop(columns=[\"init_month\", \"init_time_step\", \"production\", \"harvested_area\", \"planted_area\"]).set_index(\"adm_id\")\n",
    "\n",
    "end_of_season_df = train_df[train_df[\"init_month\"] == 12].drop(columns=[\"init_month\", \"init_time_step\", \"harvested_area\"]).set_index(\"adm_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(yhat,y):\n",
    "    return 100 * torch.sqrt(torch.mean((yhat-y)**2)) / torch.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating on year 2003\n",
      "Epoch 1, Validation Loss for year 2003: 26.678354835510255\n",
      "Epoch 2, Validation Loss for year 2003: 29.258882649739583\n",
      "Epoch 3, Validation Loss for year 2003: 24.806484858194988\n",
      "Epoch 4, Validation Loss for year 2003: 27.530549621582033\n",
      "Epoch 5, Validation Loss for year 2003: 27.721895249684653\n",
      "Epoch 6, Validation Loss for year 2003: 26.311521816253663\n",
      "Early stopping at epoch 6\n",
      "Validating on year 2004\n",
      "Epoch 1, Validation Loss for year 2004: 27.144367771763957\n",
      "Epoch 2, Validation Loss for year 2004: 27.3316252616144\n",
      "Epoch 3, Validation Loss for year 2004: 25.19689993704519\n",
      "Epoch 4, Validation Loss for year 2004: 27.0602960278911\n",
      "Epoch 5, Validation Loss for year 2004: 29.098012555030085\n",
      "Epoch 6, Validation Loss for year 2004: 29.48936293202062\n",
      "Early stopping at epoch 6\n",
      "Validating on year 2005\n",
      "Epoch 1, Validation Loss for year 2005: 42.0397554397583\n",
      "Epoch 2, Validation Loss for year 2005: 49.843784459431966\n",
      "Epoch 3, Validation Loss for year 2005: 38.166168721516925\n",
      "Epoch 4, Validation Loss for year 2005: 38.13811810811361\n",
      "Epoch 5, Validation Loss for year 2005: 40.394164911905925\n",
      "Epoch 6, Validation Loss for year 2005: 43.79479109446208\n",
      "Epoch 7, Validation Loss for year 2005: 38.68112179438273\n",
      "Early stopping at epoch 7\n",
      "Validating on year 2007\n",
      "Epoch 1, Validation Loss for year 2007: 27.679068361009872\n",
      "Epoch 2, Validation Loss for year 2007: 29.38705883707319\n",
      "Epoch 3, Validation Loss for year 2007: 26.316107273101807\n",
      "Epoch 4, Validation Loss for year 2007: 28.729466233934676\n",
      "Epoch 5, Validation Loss for year 2007: 30.56658969606672\n",
      "Epoch 6, Validation Loss for year 2007: 29.07830830982753\n",
      "Early stopping at epoch 6\n",
      "Validating on year 2008\n",
      "Epoch 1, Validation Loss for year 2008: 31.008293660481772\n",
      "Epoch 2, Validation Loss for year 2008: 31.201190694173178\n",
      "Epoch 3, Validation Loss for year 2008: 31.46621430714925\n",
      "Epoch 4, Validation Loss for year 2008: 29.62180773417155\n",
      "Epoch 5, Validation Loss for year 2008: 27.780857785542807\n",
      "Epoch 6, Validation Loss for year 2008: 31.720616213480632\n",
      "Epoch 7, Validation Loss for year 2008: 30.480283737182617\n",
      "Epoch 8, Validation Loss for year 2008: 31.280462074279786\n",
      "Early stopping at epoch 8\n",
      "Validating on year 2009\n",
      "Epoch 1, Validation Loss for year 2009: 30.687766765726025\n",
      "Epoch 2, Validation Loss for year 2009: 30.91355646067652\n",
      "Epoch 3, Validation Loss for year 2009: 29.280945679237103\n",
      "Epoch 4, Validation Loss for year 2009: 28.013285275163323\n",
      "Epoch 5, Validation Loss for year 2009: 29.97730248549889\n",
      "Epoch 6, Validation Loss for year 2009: 28.343504412420863\n",
      "Epoch 7, Validation Loss for year 2009: 28.148843962570716\n",
      "Early stopping at epoch 7\n",
      "Validating on year 2010\n",
      "Epoch 1, Validation Loss for year 2010: 25.859041477071827\n",
      "Epoch 2, Validation Loss for year 2010: 25.82412157387569\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m     47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Max Zachow\\anaconda3\\envs\\global-yield-forecast\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max Zachow\\anaconda3\\envs\\global-yield-forecast\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unique_years = end_of_season_df['harvest_year'].unique()\n",
    "unique_years.sort()\n",
    "results = dict.fromkeys(unique_years)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for year in unique_years:\n",
    "    print(f'Validating on year {year}')\n",
    "    \n",
    "    # Create training and validation sets for this fold\n",
    "    train_fold_df = end_of_season_df[end_of_season_df['harvest_year'] != year]\n",
    "    val_fold_df = end_of_season_df[end_of_season_df['harvest_year'] == year]\n",
    "    \n",
    "    train_fold_features = train_fold_df[[c for c in train_fold_df.columns if (\"tavg\" in c) or (\"tmax\" in c) or (\"tmin\" in c) or (\"prec\" in c)]]\n",
    "    train_fold_target = train_fold_df['yield']\n",
    "    val_fold_features = val_fold_df[[c for c in val_fold_df.columns if (\"tavg\" in c) or (\"tmax\" in c) or (\"tmin\" in c) or (\"prec\" in c)]]\n",
    "    val_fold_target = val_fold_df['yield']\n",
    "    \n",
    "    means = train_fold_features.mean()\n",
    "    stds = train_fold_features.std()\n",
    "    train_fold_features = (train_fold_features - means) / stds\n",
    "    val_fold_features = (val_fold_features - means) / stds\n",
    "    \n",
    "    train_fold_dataset = YieldDataset(train_fold_features, train_fold_target)\n",
    "    val_fold_dataset = YieldDataset(val_fold_features, val_fold_target)\n",
    "    \n",
    "    train_fold_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_fold_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Reset the model and optimizer\n",
    "    model = SimpleModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = RMSELoss\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    num_epochs = 10  # Set the maximum number of epochs you want to train for\n",
    "    patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    epochs_no_improve = 0  # Counter for epochs without improvement\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, target in train_fold_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features)\n",
    "            loss = criterion(output, target.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, target in val_fold_loader:\n",
    "                output = model(features)\n",
    "                loss = criterion(output, target.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_fold_loader)  # Compute the average validation loss\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss for year {year}: {val_loss}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0  # Reset the counter if validation loss improves\n",
    "        else:\n",
    "            epochs_no_improve += 1  # Increment the counter if validation loss does not improve\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break  # Stop training if no improvement for specified number of epochs\n",
    "    \n",
    "    results[year] = best_val_loss\n",
    "        \n",
    "# Once cross-validation is done, you can test on the test dataset using test_loader\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-yield-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
